{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
    "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
    "2. Tokenize text by BPE with vocab_size = 100\n",
    "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
    "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
    "5. Calculate perplexity of the language model for the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227579"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('peace.txt', 'r', encoding=\"utf-8\").read()[2:]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # TODO\n",
    "    # make lowercase\n",
    "    # replace all punctuation except dots with spaces\n",
    "    # collapse multiple spaces into one '   ' -> ' '\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\d_.]+', ' ', text)\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = preprocess_text(text)\n",
    "assert len(text) == 3141169, len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.split('.')\n",
    "text = [x.strip() for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#import nltk\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class BPE(TransformerMixin):\n",
    "    def __init__(self, vocab_size=100):\n",
    "        super(BPE, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # index to token\n",
    "        self.itos = []\n",
    "        # token to index\n",
    "        self.stoi = {}\n",
    "        \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        fit itos and stoi\n",
    "        text: list of strings \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n",
    "        # tokenize text by symbols and fill in self.itos and self.stoi\n",
    "        text = list(\" \".join(text)) \n",
    "        self.itos = [s for i, s in enumerate(set(text))]\n",
    "        self.stoi = {s:i for i, s in enumerate(self.itos)}\n",
    "        #text = # TODO\n",
    "        \n",
    "        while len(self.itos) < self.vocab_size:\n",
    "            # TODO\n",
    "            # count bigram freqencies in the text\n",
    "            bigrams = Counter([tuple(text[i:i+2]) for i in range(len(text)-1)])\n",
    "            new_token = bigrams.most_common(1)[0][0]\n",
    "            # most common bigram in the text\n",
    "            new_id = len(self.itos)\n",
    "            \n",
    "            self.itos.append(new_token)\n",
    "            self.stoi[new_token] = new_id\n",
    "            \n",
    "            # find occurences of the new_token in the text and replace them with new_id\n",
    "            upd = lambda a, b: new_id if (a, b) == new_token else a\n",
    "            end = text[-1]\n",
    "            text = [upd(*text[i:i+len(new_token)]) for i in range(len(text)-1) if tuple(text[max(i-1,0):i+len(new_token)-1]) != new_token]\n",
    "            if len(text) > 1 and (text[-2], text[-1]) != new_token:\n",
    "                text.append(end)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "        convert text to a sequence of token ids\n",
    "        text: list of strings\n",
    "        \"\"\"\n",
    "        text = [[self.stoi[s] for s in x] for x in text] # TODO tokenize text by symbols using self.stoi\n",
    "        for token_id, token in enumerate(self.itos):\n",
    "            # find occurences of the token in the text and replace them with token_id\n",
    "            if isinstance(token, str):\n",
    "                continue\n",
    "            for i, x in enumerate(text):\n",
    "                upd = lambda a, b: token_id if (self.itos[a], self.itos[b]) == token else a\n",
    "                text[i] = [upd(*x[i:i+2]) for i in range(len(x)-1) if not (i > 0 and (self.itos[x[i-1]], self.itos[x[i]]) == token)]      \n",
    "                if x and len(x) > 1 and (self.itos[x[-2]], self.itos[x[-1]]) != token:\n",
    "                    text[i].append(x[-1])\n",
    "                    \n",
    "        return text\n",
    "    \n",
    "    def decode_token(self, tok):\n",
    "        \"\"\"\n",
    "        tok: int or tuple\n",
    "        \"\"\"\n",
    "        if isinstance(tok, int):\n",
    "            if isinstance(self.itos[tok], str):\n",
    "                return self.itos[tok]\n",
    "            else:\n",
    "                return self.decode(self.itos[tok])\n",
    "        else:\n",
    "            return ''.join([x if isinstance(x, str) else self.decode(x) for x in tok])\n",
    "        \n",
    "        return result\n",
    "            \n",
    "    def decode(self, text):\n",
    "        \"\"\"\n",
    "        convert token ids into text\n",
    "        \"\"\"\n",
    "        return ''.join(map(self.decode_token, text))\n",
    "        \n",
    "        \n",
    "vocab_size = 100\n",
    "bpe = BPE(vocab_size)\n",
    "tokenized_text = bpe.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bpe.decode(tokenized_text[0]) == text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "        \n",
    "    \n",
    "start_token = vocab_size\n",
    "end_token = vocab_size + 1\n",
    "        \n",
    "    \n",
    "class LM:\n",
    "    def __init__(self, vocab_size, delta=1):\n",
    "        self.delta = delta\n",
    "        self.vocab_size = vocab_size + 2\n",
    "        self.proba = np.zeros(self.vocab_size**3)# TODO create array for storing 3-gram counters\n",
    "        \n",
    "    def infer(self, a, b, tau=1):\n",
    "        \"\"\"\n",
    "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        cut = a*self.vocab_size**2 + b*self.vocab_size\n",
    "        p = (self.proba[cut:cut+self.vocab_size] + self.delta)**(1/tau)\n",
    "        result = p / np.sum(p)\n",
    "        return result\n",
    "        \n",
    "    def get_proba(self, a, b, c, tau=1):\n",
    "        \"\"\"\n",
    "        get probability of 3-gram (a,b,c)\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        c: third token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        cut = a*self.vocab_size**2 + b*self.vocab_size\n",
    "        p = (self.proba[cut:cut+self.vocab_size] + self.delta)**(1/tau)\n",
    "        result = p[c] / np.sum(p) # TODO approximate probability by counters\n",
    "        return result\n",
    "    \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        train language model on text\n",
    "        text: list of lists\n",
    "        \"\"\"\n",
    "        cnt = Counter()\n",
    "        for i, x in enumerate(text):\n",
    "            x_trigrams = [(x[i], x[i+1], x[i+2]) for i in range(len(x)-2)]\n",
    "            cnt_ = Counter(x_trigrams)\n",
    "            cnt.update(cnt_)\n",
    "        cnt = dict(cnt)\n",
    "        trigram_to_id = lambda trigram: trigram[0]*self.vocab_size**2 + trigram[1]*self.vocab_size + trigram[2]\n",
    "        for trigram, k in cnt.items():\n",
    "            self.proba[trigram_to_id(trigram)] = k\n",
    "        # TODO count 3-grams in the text\n",
    "        \n",
    "        return self\n",
    "    \n",
    "lm = LM(vocab_size, 1).fit(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
    "    \"\"\"\n",
    "    generate sequence from language model *lm* conditioned on input_seq\n",
    "    input_seq: sequence of token ids for conditioning\n",
    "    lm: language model\n",
    "    max_len: max generated sequence length\n",
    "    k: size of beam\n",
    "    tau: temperature\n",
    "    \"\"\"\n",
    "\n",
    "    beam = [(input_seq, 0)]\n",
    "            # TODO store in beam tuples of current sequences and their log probabilities\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        candidates = []\n",
    "        candidates_proba = []\n",
    "        for snt, snt_proba in beam:\n",
    "            if np.argmax(snt[-1]) == end_token:\n",
    "                continue\n",
    "            else:    \n",
    "                proba = lm.infer(*snt[-2:], tau) # probability vector of the next token\n",
    "                best_k = proba.argsort()[-k:][::-1] # top-k most probable tokens\n",
    "                # TODO update candidates' sequences and corresponding probabilities\n",
    "                candidates += [snt + [int(x)] for x in best_k]\n",
    "                candidates_proba += [snt_proba + x for x in np.log(proba[best_k])]\n",
    "\n",
    "        beam = [(candidates[i], candidates_proba[i]) for i in np.argsort(candidates_proba).astype(int)[-k:][::-1]]\n",
    "                # select top-k most probable sequences from candidates\n",
    "    return beam\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse was she said  0.404950894628\n",
      "horse was said not  0.160088924945\n",
      "horse was she was s 0.143084119266\n",
      "horse was she cound 0.0359152963307\n",
      "horse was she cound  0.0343985108214\n",
      "horse was she count 0.0325837141851\n",
      "horse was it was sh 0.0283735835494\n",
      "horse was she was i 0.0100806863066\n",
      "horse was not been  0.00964591734276\n",
      "horse the counder and  0.00863477565772\n"
     ]
    }
   ],
   "source": [
    "input1 = 'horse '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for (snt, snt_proba) in result:\n",
    "    print(bpe.decode(snt), np.exp(snt_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "her and she said  0.306517182991\n",
      "her and with a sm 0.265585036691\n",
      "her and she was s 0.108303850541\n",
      "her and said not  0.0858167465544\n",
      "her and she cound 0.0271851614693\n",
      "her and she cound  0.0260370696199\n",
      "her and she count 0.0246634058991\n",
      "her he said not  0.0174606918544\n",
      "her and she was i 0.00763031668853\n",
      "her and with his f 0.00571154702548\n"
     ]
    }
   ],
   "source": [
    "input1 = 'her'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for (snt, snt_proba) in result:\n",
    "    print(bpe.decode(snt), np.exp(snt_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what the said not  0.000164128762823\n",
      "what the prince and  0.000112341770105\n",
      "what the could not  0.000110316090872\n",
      "what the prince and 6.91333969874e-05\n",
      "what the princess  5.46421543915e-05\n",
      "what the from the s 4.6012404354e-05\n",
      "what the from the w 4.18534009555e-05\n",
      "what the said not 4.11379190022e-05\n",
      "what the from the c 4.03939089977e-05\n",
      "what the prince of  3.79663413733e-05\n"
     ]
    }
   ],
   "source": [
    "input1 = 'what'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for (snt, snt_proba) in result:\n",
    "    print(bpe.decode(snt), np.exp(snt_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gun and she said n 0.310529906882\n",
      "gun and with a smi 0.269050396288\n",
      "gun and she was sh 0.0885500030937\n",
      "gun and said not b 0.0715415976656\n",
      "gun and she counder 0.0275408028993\n",
      "gun and she counte 0.024986460397\n",
      "gun and she cound hi 0.0240626634667\n",
      "gun and she was sa 0.0208103470787\n",
      "gun been he said  0.0144819501587\n",
      "gun and said not s 0.0138707341052\n"
     ]
    }
   ],
   "source": [
    "input1 = 'gun '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for (snt, snt_proba) in result:\n",
    "    print(bpe.decode(snt), np.exp(snt_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7347091224849125"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity(snt, lm):\n",
    "    \"\"\"\n",
    "    snt: sequence of token ids\n",
    "    lm: language model\n",
    "    \"\"\"\n",
    "    result = np.exp(-np.log(2)*np.mean([np.log(lm.get_proba(*snt[i:i+3])) for i in range(len(snt)-2)]))\n",
    "                                        #TODO perplexity for the sentence\n",
    "    return result\n",
    "\n",
    "perplexity(tokenized_text[0], lm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
