{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "assignment_7_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdPB7273pSVr"
      },
      "source": [
        "# Assignment 7\n",
        "\n",
        "Train a Transformer model for Machine Translation from Russian to English.  \n",
        "Dataset: http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz   \n",
        "Make all source and target text to lower case.  \n",
        "Use following tokenization for english:  \n",
        "```\n",
        "import sentencepiece as spm\n",
        "\n",
        "...\n",
        "spm.SentencePieceTrainer.Train('--input=data/text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "...\n",
        "TGT.build_vocab(..., min_freq=5)\n",
        "...\n",
        "\n",
        "```\n",
        "Score: corpus-bleu `nltk.translate.bleu_score.corpus_bleu`  \n",
        "Use last 1000 sentences for model evalutation (test dataset).  \n",
        "Use your target sequence tokenization for BLEU score.  \n",
        "Use max_len=50 for sequence prediction.  \n",
        "\n",
        "\n",
        "Hint: You may consider much smaller model, than shown in the example.  \n",
        "\n",
        "Baselines:  \n",
        "[4 point] BLEU = 0.05  \n",
        "[6 point] BLEU = 0.10  \n",
        "[9 point] BLEU = 0.15  \n",
        "\n",
        "[1 point] Share weights between target embeddings and output dense layer. Notice, they have the same shape.\n",
        "\n",
        "\n",
        "Readings:\n",
        "1. BLUE score how to https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "1. Transformer code and comments http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4_WCxsIpfOb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2bc99377-d409-4166-8dc7-92cadaaaaaf3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5fuE9mLdX3J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5c7164ee-2f44-4061-e2fb-38a7b9b2b1e3"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe_en.model  bpe_ru.vocab  sample_data\n",
            "bpe_en.vocab  drive\t    training-parallel-nc-v13\n",
            "bpe_ru.model  __pycache__   transformer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkjqnrOhqNRU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0cdcfcdf-10ed-4766-d4f1-7ac0492c112a"
      },
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdQOrOFrZlN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8af07a1d-4d6b-4a2f-ec19-ee30503070e3"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2jlAMg06Nj3"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from transformer import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88_pmzcbrFD4"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heVeHNLJpSVs"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from tqdm.notebook import tqdm\n",
        "from torchtext import datasets, data\n",
        "#from tqdm.notebook import tqdm\n",
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "DEVICE = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3zf-GYErxCd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f3f15717-19af-4f7a-d731-59571403f004"
      },
      "source": [
        "# opening archive in google colab\n",
        "# archive lies in drive/My Drive/Ru-En\n",
        "\n",
        "!ls drive/My\\ Drive/Ru-En"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training-parallel-nc-v13.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohnCFivNrzy2"
      },
      "source": [
        "# if using google colab\n",
        "import os, sys, tarfile\n",
        "\n",
        "tar_path = 'drive/My Drive/Ru-En/training-parallel-nc-v13.tgz'\n",
        "tar = tarfile.open(tar_path, 'r')\n",
        "\n",
        "for item in tar:\n",
        "    tar.extract(item, '.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfylUqh5sDtL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3c900eae-5db8-438e-e81e-27176882a7f0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe_en.model  bpe_ru.vocab  sample_data\n",
            "bpe_en.vocab  drive\t    training-parallel-nc-v13\n",
            "bpe_ru.model  __pycache__   transformer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfbY-54kpSVx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e888288d-c832-4879-f87e-4eda961d83f7"
      },
      "source": [
        "# tokenize english \n",
        "with open('training-parallel-nc-v13/news-commentary-v13.ru-en.en') as f:\n",
        "    with open('training-parallel-nc-v13/text.en', 'w') as out:\n",
        "            out.write(f.read().lower())\n",
        "        \n",
        "spm.SentencePieceTrainer.Train('--input=training-parallel-nc-v13/text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3ouWoigQpSV3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "04a5ecda-aeea-46aa-b123-87ee9fe030fa"
      },
      "source": [
        "# tokenize russian\n",
        "with open('training-parallel-nc-v13/news-commentary-v13.ru-en.ru') as f:\n",
        "    with open('training-parallel-nc-v13/text.ru', 'w') as out:\n",
        "            out.write(f.read().lower())\n",
        "\n",
        "spm.SentencePieceTrainer.Train('--input=training-parallel-nc-v13/text.ru --model_prefix=bpe_ru --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcXaD41SpSV7"
      },
      "source": [
        "tok_ru = spm.SentencePieceProcessor()\n",
        "tok_ru.load('bpe_ru.model')\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')\n",
        "\n",
        "SRC = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_ru.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "fields = (('src', SRC), ('tgt', TGT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efHZs5dbpSV_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ed6deedc-b187-4761-8a5c-4dc879bf33a9"
      },
      "source": [
        "with open('training-parallel-nc-v13/text.ru') as f:\n",
        "    src_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "with open('training-parallel-nc-v13/text.en') as f:\n",
        "    tgt_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "examples = [data.Example.fromlist(x, fields) for x in tqdm(zip(src_snt, tgt_snt))]\n",
        "test = data.Dataset(examples[-1000:], fields)\n",
        "train, valid = data.Dataset(examples[:-1000], fields).split(0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "235159it [00:53, 4365.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twbYFN4spSWD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "d394edf6-c820-4eea-ecde-78e587c192e4"
      },
      "source": [
        "print('src: ' + \" \".join(train.examples[100].src))\n",
        "print('tgt: ' + \" \".join(train.examples[100].tgt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src: ▁реальность ▁же ▁такова , ▁что , ▁хотя ▁берлускони ▁безусловно ▁обладает ▁своеобраз ным ▁оба я нием , ▁растущее ▁число ▁сторонников ▁трампа , ▁судя ▁по ▁всему , ▁тоже ▁находит ▁в ▁нем ▁определенное ▁оба яние , ▁даже ▁если ▁оно ▁и ▁менее ▁собла зни тельно .\n",
            "tgt: ▁the ▁reality ▁is ▁that , ▁while ▁berlusconi ▁certainly ▁has ▁his ▁charm , ▁trump ’ s ▁swelling ▁base ▁of ▁support ▁seems ▁to ▁see ▁a ▁certain ▁charm ▁in ▁him , ▁too , ▁even ▁if ▁it ▁is ▁a ▁less ▁seductive ▁version .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2zfXJRopSWM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d3cf6326-f9b3-49d2-a119-683ac08b8053"
      },
      "source": [
        "len(train), len(valid), len(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210743, 23416, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YWplTatpSWR"
      },
      "source": [
        "TGT.build_vocab(train, min_freq=5)\n",
        "SRC.build_vocab(train, min_freq=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHZUYavspSWW"
      },
      "source": [
        "from transformer import make_model, Batch\n",
        "\n",
        "    \n",
        "class BucketIteratorWrapper(DataLoader):\n",
        "    __initialized = False\n",
        "\n",
        "    def __init__(self, iterator: data.Iterator):\n",
        "#         super(BucketIteratorWrapper,self).__init__()\n",
        "        self.batch_size = iterator.batch_size\n",
        "        self.num_workers = 1\n",
        "        self.collate_fn = None\n",
        "        self.pin_memory = False\n",
        "        self.drop_last = False\n",
        "        self.timeout = 0\n",
        "        self.worker_init_fn = None\n",
        "        self.sampler = iterator\n",
        "        self.batch_sampler = iterator\n",
        "        self.__initialized = True\n",
        "\n",
        "    def __iter__(self):\n",
        "        return map(\n",
        "            lambda batch: Batch(batch.src, batch.tgt, pad=TGT.vocab.stoi['<pad>']),\n",
        "            self.batch_sampler.__iter__()\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler)\n",
        "    \n",
        "class MyCriterion(nn.Module):\n",
        "    def __init__(self, pad_idx):\n",
        "        super(MyCriterion, self).__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_idx)\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        x = x.contiguous().permute(0,2,1)\n",
        "        ntokens = (target != self.pad_idx).data.sum()\n",
        "        \n",
        "        return self.criterion(x, target) / ntokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMuu61k0x2LA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "ff960154-4ecf-405b-e723-d5dcc9d23f27"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: tokenizers==0.5.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWlF8dm7x6GA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "d33f7702-8bf8-4453-9f60-0f9cceeac0c7"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWSnLISPpSWc"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 100\n",
        "num_epochs = 10\n",
        "\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), \n",
        "                                              batch_sizes=(batch_size, batch_size, batch_size), \n",
        "                                  sort_key=lambda x: len(x.src),\n",
        "                                  shuffle=True,\n",
        "                                  device=DEVICE,\n",
        "                                  sort_within_batch=False)\n",
        "                                  \n",
        "train_iter = BucketIteratorWrapper(train_iter)\n",
        "valid_iter = BucketIteratorWrapper(valid_iter)\n",
        "test_iter = BucketIteratorWrapper(test_iter)\n",
        "\n",
        "\n",
        "#pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6, \n",
        "               d_model=256, d_ff=512, h=8, dropout=0.1)\n",
        "model = model.to(DEVICE)\n",
        "criterion = MyCriterion(pad_idx=TGT.vocab.stoi[\"<pad>\"])\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9)\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
        "#                                             num_warmup_steps=0.1*num_epochs*len(train_iter)//batch_size, \n",
        "#                                             num_training_steps=num_epochs*len(train_iter)//batch_size, \n",
        "#                                             last_epoch=-1)\n",
        "\n",
        "# share weights\n",
        "\n",
        "#model.src_embed[0].proj.weight = model.tgt_embed[0].lut.weight\n",
        "model.generator.proj.weight = model.tgt_embed[0].lut.weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTNu8ykJ1fBH"
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQJi-Md01gXv"
      },
      "source": [
        "optimizer = NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
        "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwsSlxl9pSWh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "058df7d5-5ab0-41f6-d262-d77d92ef2d09"
      },
      "source": [
        "def train_epoch(data_iter, model, criterion):\n",
        "    total_loss = 0\n",
        "    #data_iter = tqdm(data_iter)\n",
        "    counter = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        model.zero_grad()\n",
        "        out = model.forward(batch)\n",
        "        #print(torch.max(out[0], dim=-1)[1])\n",
        "        loss = criterion(out, batch.tgt_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss\n",
        "        #data_iter.set_postfix(loss = loss)\n",
        "        counter += 1\n",
        "        if i % 200 == 1:\n",
        "            print(\"Epoch Step: %d Loss: %f\" %\n",
        "                    (i, loss))\n",
        "    return total_loss / counter\n",
        "\n",
        "\n",
        "def valid_epoch(data_iter, model, criterion):\n",
        "    total_loss = 0\n",
        "    #data_iter = tqdm(data_iter)\n",
        "    counter = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        out = model.forward(batch)\n",
        "        loss = criterion(out, batch.tgt_y)\n",
        "        total_loss += loss\n",
        "        #data_iter.set_postfix(loss = loss)\n",
        "        counter += 1\n",
        "        if i % 200 == 1:\n",
        "            print(\"Epoch Step: %d Loss: %f\" %\n",
        "                    (i, loss))\n",
        "    return total_loss / counter\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loss = train_epoch(train_iter, model, criterion)\n",
        "    print('train', loss)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = valid_epoch(valid_iter, model, criterion)\n",
        "        scheduler.step(loss)\n",
        "        print('valid', loss)\n",
        "\n",
        "    torch.save(model.state_dict(), 'drive/My Drive/check.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-fafdf0561626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-fafdf0561626>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(data_iter, model, criterion)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#data_iter.set_postfix(loss = loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scheduler' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZEERkpKe5bE"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/My Drive/check.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbP1-zQ5g4Vd"
      },
      "source": [
        "start_symbol_id = TGT.vocab.stoi[\"<s>\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5T6XMvvglW6"
      },
      "source": [
        "def greedy_decode(model, src, src_mask, max_len):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(src.size(0), 1).fill_(start_symbol_id).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        prob = model.decode((ys).long(), \n",
        "                           (subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)),\n",
        "                           memory,\n",
        "                           src_mask\n",
        "                           )\n",
        "        _, next_word = torch.max(prob[:, -1, :], dim = -1)\n",
        "        ys = torch.cat([ys, next_word.unsqueeze(-1)], dim=1)\n",
        "    return ys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "venKcmc3f2FB"
      },
      "source": [
        "def beam_search(model, src, src_mask, max_len=10, k=5):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(src.size(0), 1).fill_(start_symbol_id).long().to(src.device)\n",
        "    k_beam = [(0, ys)]\n",
        "\n",
        "    for l in range(max_len):\n",
        "        all_k_beams = []\n",
        "        for prob, sent_pred in k_beam:\n",
        "            pred = model.decode(sent_pred.long(), \n",
        "                           subsequent_mask(sent_pred.size(1))\n",
        "                                    .type_as(src.data),\n",
        "                           memory,\n",
        "                           src_mask\n",
        "                           )\n",
        "            _, possible_k = torch.topk(pred[:, -1, :], k=k, dim=-1)\n",
        "\n",
        "            all_k_beams += [\n",
        "                (\n",
        "                    sum([pred[0, i, sent_pred[0, i]].item() for i in range(l)]) + pred[0, -1, next_word].item(),\n",
        "                    torch.cat([sent_pred, next_word.resize(sent_pred.size(0), 1)], dim=1)\n",
        "                )\n",
        "                for next_word in possible_k.view(k, -1)\n",
        "            ]\n",
        "\n",
        "        k_beam = sorted(all_k_beams, key=lambda x: x[0])[-k:]\n",
        "\n",
        "    return k_beam\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6V6TjvFFpSWq"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(valid_iter):\n",
        "        src = batch.src[:1]\n",
        "        src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
        "        beam = beam_search(model, src, src_key_padding_mask)\n",
        "        \n",
        "        seq = []\n",
        "        for i in range(1, src.size(1)):\n",
        "            sym = SRC.vocab.itos[src[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_ru.decode_pieces(seq)\n",
        "        print(\"\\nSource:\", seq)\n",
        "        \n",
        "        print(\"Translation:\")\n",
        "        for pred_proba, pred in beam:                \n",
        "            seq = []\n",
        "            for i in range(1, pred.size(1)):\n",
        "                sym = TGT.vocab.itos[pred[0, i]]\n",
        "                if sym == \"</s>\": break\n",
        "                seq.append(sym)\n",
        "            seq = tok_en.decode_pieces(seq)\n",
        "            print(seq)\n",
        "                \n",
        "        seq = []\n",
        "        for i in range(1, batch.tgt.size(1)):\n",
        "            sym = TGT.vocab.itos[batch.tgt[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_en.decode_pieces(seq)\n",
        "        print(\"Target:\", seq)\n",
        "        if i == 10:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcVpowgppSWt"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgGyy85qpSWy"
      },
      "source": [
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_iter):\n",
        "        #src = batch.src\n",
        "        for src, tgt in zip(batch.src, batch.tgt):\n",
        "          src = src.unsqueeze(0)\n",
        "          src_mask = (src != SRC.vocab.stoi[\"<pad>\"]).unsqueeze(-2)\n",
        "          out = beam_search(model, src, src_mask, \n",
        "                              max_len=40, k=5)\n",
        "          \n",
        "          tgt = tgt.unsqueeze(0)\n",
        "          for (prob, transl), gold in zip(out, tgt):\n",
        "              hyp = []\n",
        "              for i in range(1, transl.size(-1)):\n",
        "                  sym = TGT.vocab.itos[transl[0, i]]\n",
        "                  if sym == \"</s>\": break\n",
        "                  hyp.append(sym)\n",
        "              hypotheses.append(hyp)\n",
        "              \n",
        "              ref = []\n",
        "              for i in range(1, gold.size(-1)):\n",
        "                  sym = TGT.vocab.itos[gold.data[i]]\n",
        "                  if sym == \"</s>\": break\n",
        "                  ref.append(sym)\n",
        "              references.append([ref])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BODVP_iMVx1F"
      },
      "source": [
        "from nltk import translate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Up-fGnpSW4"
      },
      "source": [
        "corpus_bleu(references, hypotheses, \n",
        "            smoothing_function=translate.bleu_score.SmoothingFunction().method3,\n",
        "            auto_reweigh=True\n",
        "           )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}