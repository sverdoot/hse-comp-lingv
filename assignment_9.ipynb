{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7wR5v1k9IKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from zipfile import ZipFile\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import pickle\n",
        "import random\n",
        "from itertools import combinations\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9GCR6RZcBxJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ceb344d7-f695-425e-efc8-54ca07e91bc5"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppIgrMFqparQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_seed(value):\n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    np.random.seed(value)\n",
        "    random.seed(value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxZ_P7m_UN30",
        "colab_type": "text"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMqUqyYq9S74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget https://github.com/thedenaas/hse_seminars/tree/master/2018/seminar_13/data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1MqHRJi9w9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with ZipFile(\"data2.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HHNHj3d92_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "586a8b63-b0c8-4120-ede2-4efcbe31848f"
      },
      "source": [
        "raw_documents = []\n",
        "snippets = []\n",
        "with open( \"data/data.txt\", \"r\") as f:\n",
        "    for line in f.readlines():\n",
        "        text = line.strip()\n",
        "        raw_documents.append( text.lower() )\n",
        "        \n",
        "        snippets.append( text[0:min(len(text),100)] )\n",
        "print(\"Read %d raw text documents\" % len(raw_documents))\n",
        "\n",
        "\n",
        "# custom stopwords\n",
        "custom_stop_words = []\n",
        "with open( \"data/stopwords.txt\", \"r\" ) as f:\n",
        "    for line in f.readlines():\n",
        "        custom_stop_words.append( line.strip().lower() )\n",
        "        \n",
        "print(\"Stopword list has %d entries\" % len(custom_stop_words) )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 4551 raw text documents\n",
            "Stopword list has 350 entries\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpHCbSQNUUL9",
        "colab_type": "text"
      },
      "source": [
        "# Some preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz52Xc29HdY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "dataset = [tokenizer.tokenize(x) for x in raw_documents]\n",
        "\n",
        "if not Path('vocab.pickle').exists():\n",
        "    vocab_freq = {}\n",
        "    for doc in dataset:\n",
        "        for word in doc:\n",
        "            if word in vocab_freq:\n",
        "                vocab_freq[word] += 1\n",
        "            else:\n",
        "                vocab_freq[word] = 1\n",
        "else:\n",
        "    with open('vocab.pickle', 'rb') as f:\n",
        "        vocab_freq = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l31Kpir0UiL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_list = [k for k, v in vocab_freq.items() if v > 50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzG021msVDVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "dataset = [tokenizer.tokenize(x) for x in raw_documents]\n",
        "for i, doc in enumerate(dataset):\n",
        "    dataset[i] = list(filter(lambda x: x in vocab_list and x not in custom_stop_words, doc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QaGvdvaUlGM",
        "colab_type": "text"
      },
      "source": [
        "# Load pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBm0xFqWKD8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "61c25c8c-b38e-4f06-e9c5-04439cd8d97f"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-21 19:57:40--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-03-21 19:57:40--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-03-21 19:57:40--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.05MB/s    in 6m 27s  \n",
            "\n",
            "2020-03-21 20:04:07 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qv1PpFHKKK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f6d8de46-8a5c-4e46-90d8-c2eacf8aa062"
      },
      "source": [
        "import gensim\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "if not Path('emb_word2vec_format.txt').exists():\n",
        "    glove2word2vec(glove_input_file=\"glove.6B.300d.txt\", word2vec_output_file=\"emb_word2vec_format.txt\")\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('emb_word2vec_format.txt')\n",
        "weights = torch.FloatTensor(model.vectors)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "194jzguZYQCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx = {k:i for i, k in enumerate(model.vocab.keys())}\n",
        "weight = np.array([model[k] for _, k in enumerate(model.vocab.keys())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI81C_h9aodF",
        "colab_type": "text"
      },
      "source": [
        "# Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACDBvyD2EelT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encode = lambda x: word2idx[x] if x in model.vocab.keys() else word2idx['unk']\n",
        "dataset = [[encode(x) for x in y] for y in dataset]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYrjdY7VqBTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset + [[encode(x)] for x in vocab_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV-Vo6fBZQp9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e3751168-3fe5-4181-a11d-c2ad3a483196"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.hist([len(x) for x in dataset], bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqWM4SJI6hoMkqfP/ASqkasctC1EZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEMIU0S5bVpF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f2e19189-0f36-40e0-bb60-00a2bd12a868"
      },
      "source": [
        "print(len(dataset))\n",
        "print(len(list(filter(lambda x: len(x) < 1024, dataset))))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11439\n",
            "11166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HtrevTfbr_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 512\n",
        "tensor_dataset = []\n",
        "for doc in dataset:\n",
        "    tensor_dataset.append(torch.LongTensor(doc[:max_len]+(max_len - len(doc))*[encode('pad')]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yohdAUTWrSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padding_idx = word2idx['pad']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTdUSyR_autB",
        "colab_type": "text"
      },
      "source": [
        "# Batch sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOCzpQqne0ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataSampler():\n",
        "    def __init__(self, dataset, negative_size=10, batch_size=5):\n",
        "        self.dataset = random.sample(dataset, len(dataset))\n",
        "        self.negative_size = negative_size\n",
        "        self.batch_size = batch_size\n",
        "        self.cur_idx = 0\n",
        "        self.seed = 0 \n",
        "\n",
        "    def __next__(self):\n",
        "        batch = {'positive': [], 'negative': []}\n",
        "        for j in range(self.batch_size):\n",
        "            positive = self.dataset[self.cur_idx]\n",
        "            rest = self.dataset[:self.batch_size*self.cur_idx+j] + self.dataset[self.batch_size*self.cur_idx+j+1:]\n",
        "            random.seed(self.seed + j)\n",
        "            negative = torch.stack(random.sample(rest, self.negative_size), 0)\n",
        "            batch['positive'].append(positive)\n",
        "            batch['negative'].append(negative)\n",
        "        batch['positive'] = torch.stack(batch['positive'], 0)\n",
        "        batch['negative'] = torch.stack(batch['negative'], 0)\n",
        "          \n",
        "        return batch\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.cur_idx = 0\n",
        "        for i in range(len(self.dataset) // self.batch_size):\n",
        "            self.cur_idx += 1\n",
        "            self.seed = i * self.batch_size\n",
        "            yield self.__next__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) // self.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQeqN5CLayry",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdFohVtEm-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TopicModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d, n_topics):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d = d\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d)\n",
        "        self.M_matrix = nn.Linear(d, d, bias=False)\n",
        "        self.proj = nn.Linear(d, n_topics)\n",
        "\n",
        "        self.T_matrix = nn.Parameter(nn.init.xavier_uniform_(torch.empty(n_topics, d)))\n",
        "\n",
        "    def load_embedding_weight(self, weight, padding_idx=None, freeze=False):\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.d).from_pretrained(weight, padding_idx=padding_idx)\n",
        "        if freeze == True:\n",
        "            self.embedding.requires_grad = False\n",
        "\n",
        "    def forward(self, batch):\n",
        "        pos_emb = self.embedding(batch['positive'])\n",
        "        neg_context_emb = self.embedding(batch['negative']).mean(2)\n",
        "\n",
        "        sent_context = pos_emb.mean(1)\n",
        "        transf_emb = self.M_matrix(pos_emb)\n",
        "        sim = torch.einsum('ble,be->bl', transf_emb, sent_context)\n",
        "        alphas = F.softmax(sim, -1)\n",
        "        attn = torch.einsum('ble,bl->be', pos_emb, alphas)\n",
        "        p = F.softmax(self.proj(attn), -1)\n",
        "        r = p @ self.T_matrix\n",
        "        \n",
        "        pos = torch.einsum('be,be->b', r, attn)\n",
        "        neg = torch.einsum('be,bme->bm', r, neg_context_emb)\n",
        "\n",
        "        return pos, neg \n",
        "    \n",
        "    def get_probs(self, inp):\n",
        "        pos_emb = self.embedding(inp)\n",
        "\n",
        "        sent_context = pos_emb.mean(0)\n",
        "        transf_emb = self.M_matrix(pos_emb)\n",
        "        sim = torch.einsum('le,e->l', transf_emb, sent_context)\n",
        "        alphas = F.softmax(sim, -1)\n",
        "        attn = torch.einsum('le,l->e', pos_emb, alphas)\n",
        "        p = F.softmax(self.proj(attn))\n",
        "\n",
        "        return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pa2B7OAa09N",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQMh7sngRl7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epoch = 15\n",
        "batch_size = 10\n",
        "negative_size = 20\n",
        "lamda = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7eZKLP3McEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_topics_range = [3, 4, 5, 6, 7, 8, 9, 10]\n",
        "topic_models = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLHF7IEmwEJL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "754cbf21-671d-42c1-9dda-bb73cdc94576"
      },
      "source": [
        "for n_topics in n_topics_range:\n",
        "    print(f'{n_topics} topics \\n')\n",
        "    random_seed(999)\n",
        "    topic_model = TopicModel(len(word2idx), 300, n_topics=n_topics)\n",
        "    topic_model.load_embedding_weight(torch.FloatTensor(weight), padding_idx=padding_idx, freeze=True)\n",
        "    topic_model.to(device)\n",
        "    optimizer = Adam(topic_model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n",
        "\n",
        "    def topic_loss_function(pos, neg, model):\n",
        "        pos = pos.unsqueeze(-1)\n",
        "        delta = 1 - pos + neg\n",
        "        delta = F.relu(delta)\n",
        "        reg = torch.frobenius_norm(model.T_matrix @ model.T_matrix.permute(1,0)  - torch.eye(n_topics).to(pos.device))\n",
        "        loss = delta.sum() / batch_size + lamda * reg / len(sampler) / batch_size\n",
        "        return loss\n",
        "\n",
        "    topic_model.train()\n",
        "    for ep in range(n_epoch):\n",
        "        ep_loss = 0\n",
        "        sampler = DataSampler(tensor_dataset, negative_size=negative_size, batch_size=batch_size)\n",
        "        for step, batch in enumerate(iter(sampler)):\n",
        "            for k, v in batch.items():\n",
        "                batch[k] = v.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pos, neg = topic_model(batch)\n",
        "            loss = topic_loss_function(pos, neg, topic_model)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            ep_loss += loss.item()\n",
        "        scheduler.step(ep_loss)\n",
        "        print(f'Epoch {ep}, loss {ep_loss / len(sampler)}')\n",
        "\n",
        "    topic_model.eval()\n",
        "    with torch.no_grad():\n",
        "        W = []\n",
        "        for x in tensor_dataset:\n",
        "            W.append(topic_model.get_probs(x.to(device)).cpu().numpy())\n",
        "        W = np.array(W)\n",
        "\n",
        "        H = []\n",
        "        for x in vocab_list:\n",
        "            if x not in word2idx.keys():\n",
        "                continue\n",
        "            H.append(topic_model.get_probs(torch.tensor([word2idx[x]]).to(device)).cpu().numpy())\n",
        "        H = np.array(H)\n",
        "        H = H.transpose()\n",
        "\n",
        "    topic_models.append((n_topics, W, H))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 topics \n",
            "\n",
            "Epoch 0, loss 1.1834207667004717\n",
            "Epoch 1, loss 0.327033816035235\n",
            "Epoch 2, loss 0.10593561906500354\n",
            "Epoch 3, loss 0.04254377763309945\n",
            "Epoch 4, loss 0.014832768478444659\n",
            "Epoch 5, loss 0.005192565622978639\n",
            "Epoch 6, loss 0.002252294764363501\n",
            "Epoch 7, loss 0.001550640515479555\n",
            "Epoch 8, loss 0.0013125151410464226\n",
            "Epoch 9, loss 0.0014198245541767029\n",
            "Epoch 10, loss 0.0016580137072989842\n",
            "Epoch 11, loss 0.0014127142836762657\n",
            "Epoch 12, loss 0.001256457536174954\n",
            "Epoch 13, loss 0.0012029627925119089\n",
            "Epoch 14, loss 0.001464898186232259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4 topics \n",
            "\n",
            "Epoch 0, loss 1.3867467337331045\n",
            "Epoch 1, loss 0.38208580567142053\n",
            "Epoch 2, loss 0.12283563132219388\n",
            "Epoch 3, loss 0.04741645472271699\n",
            "Epoch 4, loss 0.0180129351969568\n",
            "Epoch 5, loss 0.0064925968365182355\n",
            "Epoch 6, loss 0.002409546986360377\n",
            "Epoch 7, loss 0.001572645325250337\n",
            "Epoch 8, loss 0.0012528051808394967\n",
            "Epoch 9, loss 0.0012887710023407391\n",
            "Epoch 10, loss 0.0013052094522016228\n",
            "Epoch 11, loss 0.0013639968078443332\n",
            "Epoch 12, loss 0.001349588274635531\n",
            "Epoch 13, loss 0.0014078721298354622\n",
            "Epoch 14, loss 0.0013813458099734765\n",
            "5 topics \n",
            "\n",
            "Epoch 0, loss 6.89416758484745\n",
            "Epoch 1, loss 6.778068684120867\n",
            "Epoch 2, loss 6.68103436588726\n",
            "Epoch 3, loss 6.513629790242416\n",
            "Epoch 4, loss 6.229259419245181\n",
            "Epoch 5, loss 12.883892161941134\n",
            "Epoch 6, loss 2.1724609828709722\n",
            "Epoch 7, loss 0.4892831161725787\n",
            "Epoch 8, loss 0.3263258588853132\n",
            "Epoch 9, loss 0.21273692985238835\n",
            "Epoch 10, loss 0.13854386920821532\n",
            "Epoch 11, loss 0.08997674391475406\n",
            "Epoch 12, loss 0.058244637396978584\n",
            "Epoch 13, loss 0.03857686960367341\n",
            "Epoch 14, loss 0.023753591532039205\n",
            "6 topics \n",
            "\n",
            "Epoch 0, loss 1.1734559487718028\n",
            "Epoch 1, loss 0.2941389963620938\n",
            "Epoch 2, loss 0.09080135611714779\n",
            "Epoch 3, loss 0.037846065866559246\n",
            "Epoch 4, loss 0.016780264347148792\n",
            "Epoch 5, loss 0.015880178511044955\n",
            "Epoch 6, loss 0.014980371215760428\n",
            "Epoch 7, loss 0.012745072532482674\n",
            "Epoch 8, loss 0.012399265460432886\n",
            "Epoch 9, loss 0.01233760211243981\n",
            "Epoch 10, loss 0.0017632709746636052\n",
            "Epoch 11, loss 0.002833087510699838\n",
            "Epoch 12, loss 0.0015372894786840935\n",
            "Epoch 13, loss 0.0014440585963618685\n",
            "Epoch 14, loss 0.001391441876815073\n",
            "7 topics \n",
            "\n",
            "Epoch 0, loss 10.979138614705551\n",
            "Epoch 1, loss 10.566186776471895\n",
            "Epoch 2, loss 10.177375843133069\n",
            "Epoch 3, loss 9.495233410073498\n",
            "Epoch 4, loss 8.447981953780612\n",
            "Epoch 5, loss 7.048624717918072\n",
            "Epoch 6, loss 5.421309917656768\n",
            "Epoch 7, loss 3.7050111262504197\n",
            "Epoch 8, loss 2.0239488273102886\n",
            "Epoch 9, loss 1.0987989248044638\n",
            "Epoch 10, loss 8.824308202596265\n",
            "Epoch 11, loss 0.9253452541216987\n",
            "Epoch 12, loss 0.9130522623358331\n",
            "Epoch 13, loss 0.8956313914409981\n",
            "Epoch 14, loss 0.8851794153582289\n",
            "8 topics \n",
            "\n",
            "Epoch 0, loss 1.2978471203561817\n",
            "Epoch 1, loss 0.28033370287663795\n",
            "Epoch 2, loss 0.05737565116916497\n",
            "Epoch 3, loss 0.014103800224075917\n",
            "Epoch 4, loss 0.005300998536394037\n",
            "Epoch 5, loss 0.0021545403492975123\n",
            "Epoch 6, loss 0.0016171825094937248\n",
            "Epoch 7, loss 0.0014098117307068051\n",
            "Epoch 8, loss 0.0013220986338741327\n",
            "Epoch 9, loss 0.0012261304848119356\n",
            "Epoch 10, loss 0.0018580002879906422\n",
            "Epoch 11, loss 0.0012981156713255109\n",
            "Epoch 12, loss 0.001143284655242917\n",
            "Epoch 13, loss 0.0018365642516030454\n",
            "Epoch 14, loss 0.0013823682931801817\n",
            "9 topics \n",
            "\n",
            "Epoch 0, loss 10.867387302978425\n",
            "Epoch 1, loss 10.478558070589775\n",
            "Epoch 2, loss 10.430912928479339\n",
            "Epoch 3, loss 9.769544132988026\n",
            "Epoch 4, loss 9.3356333323074\n",
            "Epoch 5, loss 8.646601358340599\n",
            "Epoch 6, loss 7.647637225723355\n",
            "Epoch 7, loss 6.349889761324213\n",
            "Epoch 8, loss 4.828392583502038\n",
            "Epoch 9, loss 3.182904237315828\n",
            "Epoch 10, loss 1.6150380079671154\n",
            "Epoch 11, loss 0.9459395500410052\n",
            "Epoch 12, loss 0.8590506280813943\n",
            "Epoch 13, loss 0.826150306879379\n",
            "Epoch 14, loss 0.7970006939933056\n",
            "10 topics \n",
            "\n",
            "Epoch 0, loss 1.7368104578012287\n",
            "Epoch 1, loss 0.29480135673911684\n",
            "Epoch 2, loss 0.0652732806180849\n",
            "Epoch 3, loss 0.018548975543633155\n",
            "Epoch 4, loss 0.005886632845022456\n",
            "Epoch 5, loss 0.0020997534882463025\n",
            "Epoch 6, loss 0.0016320478693604118\n",
            "Epoch 7, loss 0.0014105624295624457\n",
            "Epoch 8, loss 0.0012919758723548443\n",
            "Epoch 9, loss 0.0015250401035047357\n",
            "Epoch 10, loss 0.0015578217109199206\n",
            "Epoch 11, loss 0.0013433699838308682\n",
            "Epoch 12, loss 0.0015931271992530918\n",
            "Epoch 13, loss 0.0015044129629393718\n",
            "Epoch 14, loss 0.0012158083407394099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oco_UCl3a4PP",
        "colab_type": "text"
      },
      "source": [
        "# Coherence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wStKGxoms8U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terms = list(filter(lambda x: x in word2idx.keys(), vocab_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NUOC-drjLrQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c0cfff3-a6b4-4c47-d5f4-319eb33413e7"
      },
      "source": [
        "def get_descriptor( terms, H, topic_index, top ):\n",
        "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
        "    top_terms = []\n",
        "    for term_index in top_indices[0:top]:\n",
        "        top_terms.append( terms[term_index] )\n",
        "    return top_terms\n",
        "\n",
        "for (k,_,H) in topic_models:\n",
        "    print(f'{k} topics')\n",
        "    descriptors = []\n",
        "    for topic_index in range(k):\n",
        "        descriptors.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
        "        str_descriptor = \", \".join( descriptors[topic_index] )\n",
        "        print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )\n",
        "    print('\\n')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 topics\n",
            "Topic 01: urged, reforms, urges, urge, ministers, policymakers, government, leaders, allies, austerity\n",
            "Topic 02: dirty, tears, sings, guy, laughing, funny, dad, jokes, loud, oh\n",
            "Topic 03: #, download, x, entries, category, chart, server, %, user, multiple\n",
            "\n",
            "\n",
            "4 topics\n",
            "Topic 01: ministers, eu, opposition, nations, islamic, bloc, against, treaty, parliament, independence\n",
            "Topic 02: customer, customers, quality, software, mortgages, data, you, information, appreciate, buy\n",
            "Topic 03: organised, belfast, deutsche, klopp, mass, wembley, midlands, burnley, paschi, trafford\n",
            "Topic 04: &, aka, bass, #, guitarist, j, starred, drummer, drums, starring\n",
            "\n",
            "\n",
            "5 topics\n",
            "Topic 01: hit, ball, playing, shot, hitting, coach, scoring, match, players, season\n",
            "Topic 02: shouldn, wouldn, hasn, iphone, couldn, weren, wasn, download, doesn, didn\n",
            "Topic 03: gea, lloris, far-right, giroud, pochettino, fca, me., lamela, hinkley, negredo\n",
            "Topic 04: (, ), josé, –, born, %, 2013, martínez, dec, 6\n",
            "Topic 05: thinktank, urged, pledged, vowed, independence, parliament, constitutional, referendum, minister, ministers\n",
            "\n",
            "\n",
            "6 topics\n",
            "Topic 01: edward, sir, son, daughter, father, brother, mary, margaret, ali, wife\n",
            "Topic 02: ambition, ultimate, terrifying, stupid, trump, obsession, habit, cruel, ridiculous, yourself\n",
            "Topic 03: oct, g, nov, w, c, b, aug, apr, gp, 3\n",
            "Topic 04: bony, landing, thin, shoulder, brake, spaces, surface, thick, tiny, comfortably\n",
            "Topic 05: language, film-makers, literature, far-right, obr, fca, bhs, long., pogba, greek\n",
            "Topic 06: government, economic, governments, leaders, security, pledged, country, cooperation, policy, issues\n",
            "\n",
            "\n",
            "7 topics\n",
            "Topic 01: fines, penalties, holocaust, scandal, nazi, fraud, illegal, fee, fees, tax\n",
            "Topic 02: hasn, shouldn, revenant, firmino, wasn, melania, startup, couldn, uber, yellen\n",
            "Topic 03: birthday, bonuses, ftse, annual, exclusive, *, opens, writers, ten, deutsche\n",
            "Topic 04: balls, shoulder, protesters, surgery, seats, finger, leg, brake, injured, swing\n",
            "Topic 05: nuclear, merkel, programme, visa, na, passwords, programmes, sung, id, bah\n",
            "Topic 06: schmeichel, obligation, connections, loyalty, respect, recognise, cross, richmond, marine, port\n",
            "Topic 07: findings, humans, theories, brain, secondary, disease, fossil, diseases, overwhelmingly, emotions\n",
            "\n",
            "\n",
            "8 topics\n",
            "Topic 01: cinematic, musical, artistic, passion, visual, pure, taste, surreal, theatrical, expression\n",
            "Topic 02: deserve, please, myself, everybody, cares, honestly, somebody, you, anybody, ourselves\n",
            "Topic 03: fa, pulis, villa, champions, mourinho, europa, moyes, barcelona, football, firmino\n",
            "Topic 04: town, john, dean, died, duncan, henry, jon, sheriff, ian, richard\n",
            "Topic 05: (, ), #, l, dec, –, m, c, et, x\n",
            "Topic 06: sexually, rihanna, rape, emails, ivanka, tweet, beyoncé, fgm, porn, tweeted\n",
            "Topic 07: economic, reforms, tensions, relations, cooperation, crisis, amid, reform, coalition, democracy\n",
            "Topic 08: yards, scoring, minutes, cap, four, balls, two, seconds, five, minute\n",
            "\n",
            "\n",
            "9 topics\n",
            "Topic 01: degree, massachusetts, law, insurance, tax, hull, d, boat, penalties, regulators\n",
            "Topic 02: shares, gold, ftse, iran, x, stocks, analyst, prices, bid, s\n",
            "Topic 03: rally, gp, 1-0, index, free, header, drive, wall, points, ftse\n",
            "Topic 04: yoga, landing, brake, leg, keyboard, wan, pen, fingers, vaccine, launching\n",
            "Topic 05: paschi, passwords, firmino, relaxed, virtually, intelligence, comey, engaging, password, illegally\n",
            "Topic 06: agüero, film-maker, peaches, lallana, flowers, buzzfeed, states., 1., loyalty, benítez\n",
            "Topic 07: yellen, disorders, harmful, pochettino, fossil, kasich, processes, disorder, pressures, experiencing\n",
            "Topic 08: barriers, countless, bony, lallana, first-half, occasions, tadic, lukaku, arsène, instagram\n",
            "Topic 09: t, l, h, ', ’, france, m, urges, ‘, courage\n",
            "\n",
            "\n",
            "10 topics\n",
            "Topic 01: billion, stocks, million, industrial, shares, exports, largest, mining, export, products\n",
            "Topic 02: minister, adviser, al, spokesman, denied, aide, premier, confirmed, deputy, leader\n",
            "Topic 03: guitar, album, songs, vocals, song, sings, genre, soundtrack, soul, lyrics\n",
            "Topic 04: ), (, vice-president, ’, published, ', 1968, founding, press, 1976\n",
            "Topic 05: paschi, pulis, states., said., agüero, clyne, hotspur, lamela, guardiola, thorburn\n",
            "Topic 06: corner, streets, map, rain, empty, walls, square, wind, tidal, north-east\n",
            "Topic 07: dem, jp, *, lib, wasn, didn, breitbart, wouldn, bah, @\n",
            "Topic 08: unfair, urge, accountable, deserve, excuse, honest, ourselves, reassure, credibility, ignore\n",
            "Topic 09: whitehall, arsène, darmian, asic, long., trolls, mané, pickford, in., comey\n",
            "Topic 10: websites, trump, wi-fi, facebook, third-party, website, netflix, app, far-right, google\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6essmo3jP-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_top_snippets( all_snippets, W, topic_index, top ):\n",
        "#     top_indices = np.argsort( W[:,topic_index] )[::-1]\n",
        "#     top_snippets = []\n",
        "#     for doc_index in top_indices[0:top]:\n",
        "#         top_snippets.append( all_snippets[doc_index] )\n",
        "#     return top_snippets\n",
        "\n",
        "# for (k,W,H) in topic_models:\n",
        "#     print(f'{k} topics')\n",
        "#     topic_snippets = get_top_snippets( snippets, W, 0, 10 )\n",
        "#     for i, snippet in enumerate(topic_snippets):\n",
        "#         print(\"%02d. %s\" % ( (i+1), snippet ) )\n",
        "#     print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1G8fwkaUKJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "7e3a6c50-5c2f-4769-82f4-498e8e01ad8b"
      },
      "source": [
        "def calculate_coherence( w2v_model, term_rankings):\n",
        "    overall_coherence = 0.0\n",
        "    for topic_index in range(len(term_rankings)):\n",
        "        # check each pair of terms\n",
        "        pair_scores = []\n",
        "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
        "            pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n",
        "        # get the mean for all pairs in this topic\n",
        "        topic_score = sum(pair_scores) / len(pair_scores)\n",
        "        overall_coherence += topic_score\n",
        "    # get the mean score across all topics\n",
        "    return overall_coherence / len(term_rankings)\n",
        "\n",
        "k_values = []\n",
        "coherences = []\n",
        "for (k,W,H) in topic_models:\n",
        "    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n",
        "    term_rankings = []\n",
        "    for topic_index in range(k):\n",
        "        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
        "    # Now calculate the coherence based on our Word2vec model\n",
        "    k_values.append( k )\n",
        "    coherences.append( calculate_coherence( model, term_rankings ) )\n",
        "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K=03: Coherence=0.3121\n",
            "K=04: Coherence=0.2731\n",
            "K=05: Coherence=0.2948\n",
            "K=06: Coherence=0.2827\n",
            "K=07: Coherence=0.1682\n",
            "K=08: Coherence=0.3148\n",
            "K=09: Coherence=0.1345\n",
            "K=10: Coherence=0.2678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7W4JBKqZiiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}